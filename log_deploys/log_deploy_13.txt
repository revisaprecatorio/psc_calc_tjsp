Agora vamos analisar o problema do Chrome. Vou verificar o crawler_full.py:

Encontrei! O 
crawler_full.py
 ainda aceita e usa o argumento --user-data-dir internamente. Mesmo que o orchestrator n√£o passe mais, o crawler pode estar pegando de outra fonte (vari√°vel de ambiente ou valor padr√£o).

Vamos verificar se h√° vari√°vel de ambiente sendo usada:

Searched
CHROME_USER_DATA_DIR in crawler_full.py (0)
Agora vamos ver onde user_data_dir √© definido na fun√ß√£o principal:

Perfeito! Agora entendi completamente o problema. O 
crawler_full.py
 recebe args.user_data_dir que pode ser None, mas o Chrome ainda est√° tentando usar algum diret√≥rio padr√£o.

Vamos verificar se h√° algum valor padr√£o sendo usado quando user_data_dir √© None:

‚úÖ Documenta√ß√£o Atualizada e Problema Identificado!
üìö O Que Foi Feito:
1. DEPLOY_TRACKING.md Atualizado:
‚úÖ Adicionado Problema #6: Selenium n√£o baixava PDFs (resolvido)
‚úÖ Adicionado Problema #7: Chrome user-data-dir em uso (em investiga√ß√£o)
‚úÖ Documentadas todas as tentativas e solu√ß√µes
2. Logs de Deploy Adicionados ao Git:
‚úÖ 12 arquivos de log detalhados (
log_deploy_1.txt
 at√© 
log_deploy_12.txt
)
‚úÖ Token sens√≠vel removido do log_deploy_11.txt
‚úÖ Commit e push realizados com sucesso
üîç AN√ÅLISE DO PROBLEMA - Chrome user-data-dir
Descoberta:
O 
crawler_full.py
 ainda est√° usando user-data-dir internamente, mesmo que o orchestrator n√£o passe mais o argumento!

C√≥digo problem√°tico:

python
# crawler_full.py - Linha 134-136
if user_data_dir:
    opts.add_argument(f"--user-data-dir={user_data_dir}")
    opts.add_argument("--profile-directory=Default")
Causa Raiz:
Quando user_data_dir=None (que √© o caso agora), o Chrome N√ÉO deveria adicionar o argumento. Mas o erro persiste, o que indica uma de duas coisas:

Hip√≥tese A: H√° um processo antigo do Chrome ainda rodando no container
Hip√≥tese B: O Chrome est√° usando um diret√≥rio padr√£o do sistema que est√° bloqueado
‚úÖ Solu√ß√£o Proposta:
Op√ß√£o 1: Limpar Processos do Chrome no Container
Adicionar um comando para matar processos do Chrome antes de cada execu√ß√£o:

python
# No orchestrator_subprocess.py, antes de executar o crawler:
import subprocess
subprocess.run(["pkill", "-9", "chrome"], capture_output=True)
subprocess.run(["pkill", "-9", "chromium"], capture_output=True)
Op√ß√£o 2: For√ßar Diret√≥rio Tempor√°rio √önico no Crawler
Modificar o 
crawler_full.py
 para sempre criar um diret√≥rio tempor√°rio:

python
# crawler_full.py - Linha 132-136
def make_options():
    opts = Options()
    if user_data_dir:
        opts.add_argument(f"--user-data-dir={user_data_dir}")
    else:
        # NOVO: Criar diret√≥rio tempor√°rio √∫nico
        import tempfile
        temp_dir = tempfile.mkdtemp(prefix="chrome_")
        opts.add_argument(f"--user-data-dir={temp_dir}")
    
    opts.add_argument("--profile-directory=Default")
Op√ß√£o 3: Adicionar Flag --disable-dev-shm-usage Mais Agressiva
python
opts.add_argument("--disable-dev-shm-usage")
opts.add_argument("--disable-software-rasterizer")
opts.add_argument("--disable-extensions")
opts.add_argument("--disable-background-networking")
üéØ Recomenda√ß√£o:
Vamos com a Op√ß√£o 2 - modificar o 
crawler_full.py
 para criar um diret√≥rio tempor√°rio √∫nico quando user_data_dir n√£o for fornecido.

Quer que eu implemente essa solu√ß√£o? üöÄ

Perfeito! Vou implementar a Op√ß√£o 2 - modificar o 
crawler_full.py
 para criar um diret√≥rio tempor√°rio √∫nico quando user_data_dir n√£o for fornecido.

crawler_full.py
+8
Perfeito! Agora vamos fazer commit e push:


‚úÖ CORRE√á√ÉO IMPLEMENTADA COM SUCESSO!
üîß O Que Foi Modificado:
crawler_full.py - Linha 137-144:
python
# ANTES
if user_data_dir:
    opts.add_argument(f"--user-data-dir={user_data_dir}")
    opts.add_argument("--profile-directory=Default")

# AGORA
if user_data_dir:
    opts.add_argument(f"--user-data-dir={user_data_dir}")
    opts.add_argument("--profile-directory=Default")
else:
    # CORRIGIDO: Criar diret√≥rio tempor√°rio √∫nico
    import tempfile
    import time
    temp_dir = tempfile.mkdtemp(prefix=f"chrome_{int(time.time())}_")
    opts.add_argument(f"--user-data-dir={temp_dir}")
    opts.add_argument("--profile-directory=Default")
üéØ Como Funciona:
‚úÖ Se user_data_dir for fornecido ‚Üí usa ele
‚úÖ Se user_data_dir for None ‚Üí cria diret√≥rio tempor√°rio √∫nico
‚úÖ Cada execu√ß√£o tem seu pr√≥prio diret√≥rio: /tmp/chrome_1727760000_abc123/
‚úÖ Sem conflitos entre execu√ß√µes paralelas ou sequenciais
üöÄ Pr√≥ximos Passos no Servidor:
bash
# 1. Parar o worker
docker compose down

# 2. Atualizar c√≥digo
git pull origin main

# 3. Rebuild da imagem
docker compose build

# 4. Resetar um job para teste
psql -h 72.60.62.124 -p 5432 -U admin -d n8n -c "UPDATE consultas_esaj SET status = FALSE WHERE id = 27;"

# 5. Subir e ver logs
docker compose up
üìä O Que Deve Acontecer Agora:
‚úÖ Chrome cria diret√≥rio tempor√°rio √∫nico
‚úÖ Selenium inicia sess√£o sem erro
‚úÖ Crawler acessa o site do TJSP
‚úÖ PDFs s√£o baixados (se n√£o houver certificado digital ou outros problemas)
Execute os comandos acima e me mostre o output! Esta deve ser a solu√ß√£o definitiva! üéâ

---- EXEC ----

root@srv987902:/opt/crawler_tjsp# docker compose down
WARN[0000] /opt/crawler_tjsp/docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion 
[+] Running 2/2
 ‚úî Container tjsp_worker_1       Removed                                         0.0s 
 ‚úî Network crawler_tjsp_default  Removed                                         0.1s 
root@srv987902:/opt/crawler_tjsp# git pull origin main
remote: Enumerating objects: 21, done.
remote: Counting objects: 100% (21/21), done.
remote: Compressing objects: 100% (8/8), done.
remote: Total 18 (delta 10), reused 18 (delta 10), pack-reused 0 (from 0)
Unpacking objects: 100% (18/18), 22.00 KiB | 2.00 MiB/s, done.
From https://github.com/revisaprecatorio/crawler_tjsp
 * branch            main       -> FETCH_HEAD
   dc5bf3e..33a7c78  main       -> origin/main
Updating dc5bf3e..33a7c78
Fast-forward
 DEPLOY_TRACKING.md            |   65 +++
 crawler_full.py               |    8 +
 log_deploys/log_deploy_1.txt  | 1009 +++++++++++++++++++++++++++++++++++++++++++++++
 log_deploys/log_deploy_10.txt |  139 +++++++
 log_deploys/log_deploy_11.txt |  178 +++++++++
 log_deploys/log_deploy_12.txt |  207 ++++++++++
 log_deploys/log_deploy_13.txt |   90 +++++
 log_deploys/log_deploy_2.txt  |  980 +++++++++++++++++++++++++++++++++++++++++++++
 log_deploys/log_deploy_3.txt  |    0
 log_deploys/log_deploy_4.txt  |   73 ++++
 log_deploys/log_deploy_5.txt  |  255 ++++++++++++
 log_deploys/log_deploy_6.txt  |  332 ++++++++++++++++
 log_deploys/log_deploy_7.txt  |  155 ++++++++
 log_deploys/log_deploy_8.txt  |  270 +++++++++++++
 log_deploys/log_deploy_9.txt  |  101 +++++
 15 files changed, 3862 insertions(+)
 create mode 100644 log_deploys/log_deploy_1.txt
 create mode 100644 log_deploys/log_deploy_10.txt
 create mode 100644 log_deploys/log_deploy_11.txt
 create mode 100644 log_deploys/log_deploy_12.txt
 create mode 100644 log_deploys/log_deploy_13.txt
 create mode 100644 log_deploys/log_deploy_2.txt
 create mode 100644 log_deploys/log_deploy_3.txt
 create mode 100644 log_deploys/log_deploy_4.txt
 create mode 100644 log_deploys/log_deploy_5.txt
 create mode 100644 log_deploys/log_deploy_6.txt
 create mode 100644 log_deploys/log_deploy_7.txt
 create mode 100644 log_deploys/log_deploy_8.txt
 create mode 100644 log_deploys/log_deploy_9.txt
root@srv987902:/opt/crawler_tjsp# docker compose build
WARN[0000] /opt/crawler_tjsp/docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion 
#1 [internal] load local bake definitions
#1 reading from stdin 496B done
#1 DONE 0.0s

#2 [internal] load build definition from Dockerfile
#2 transferring dockerfile: 1.19kB done
#2 DONE 0.0s

#3 [internal] load metadata for docker.io/library/python:3.12-slim-bookworm
#3 DONE 0.8s

#4 [internal] load .dockerignore
#4 transferring context: 2B done
#4 DONE 0.0s

#5 [1/7] FROM docker.io/library/python:3.12-slim-bookworm@sha256:25c2d6d3b4680597f42caabe8b3ade05a60bba86547cb219356d5774ab319a8a
#5 DONE 0.0s

#6 [internal] load build context
#6 transferring context: 405.17kB 0.0s done
#6 DONE 0.0s

#7 [5/7] RUN pip install --upgrade pip
#7 CACHED

#8 [3/7] WORKDIR /app
#8 CACHED

#9 [2/7] RUN apt-get update && apt-get install -y --no-install-recommends     wget ca-certificates gnupg2 unzip fonts-liberation     libnss3-tools openssl chromium     libnss3 libxss1 libasound2 libatk1.0-0 libgtk-3-0 libgbm1 libx11-xcb1   && rm -rf /var/lib/apt/lists/*
#9 CACHED

#10 [4/7] COPY requirements.txt /app/
#10 CACHED

#11 [6/7] RUN pip install -r requirements.txt
#11 CACHED

#12 [7/7] COPY . /app/
#12 DONE 0.1s

#13 exporting to image
#13 exporting layers 0.0s done
#13 writing image sha256:0b91dae7e9bfe663c8deebee8dd6315612fe224d129ea342762feb419d415f16 done
#13 naming to docker.io/library/tjsp-worker:latest done
#13 DONE 0.1s

#14 resolving provenance for metadata file
#14 DONE 0.0s
[+] Building 1/1
 ‚úî tjsp-worker:latest  Built                                                     0.0s 
root@srv987902:/opt/crawler_tjsp# psql -h 72.60.62.124 -p 5432 -U admin -d n8n -c "UPDATE consultas_esaj SET status = FALSE WHERE id = 27;"
Password for user admin: 
UPDATE 1
root@srv987902:/opt/crawler_tjsp# docker compose up
WARN[0000] /opt/crawler_tjsp/docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion 
[+] Running 2/2
 ‚úî Network crawler_tjsp_default  Created                                         0.0s 
 ‚úî Container tjsp_worker_1       Created                                         0.0s 
Attaching to tjsp_worker_1
tjsp_worker_1  | Conectando ao banco de dados PostgreSQL...
tjsp_worker_1  | Executando a query para buscar o pr√≥ximo item da fila...
tjsp_worker_1  | Item encontrado para processar: ID=27, CPF=07620857893
tjsp_worker_1  | Conex√£o com o banco de dados fechada.
tjsp_worker_1  | 
tjsp_worker_1  | ================================================================================
tjsp_worker_1  | Processando item 1/1 do Job ID=27: Processo 0077044-50.2023.8.26.0500
tjsp_worker_1  | ================================================================================
tjsp_worker_1  | Executando comando: /usr/local/bin/python crawler_full.py --doc 0077044-50.2023.8.26.0500 --abrir-autos --baixar-pdf --turbo-download --download-dir /app/downloads/07620857893
tjsp_worker_1  | 
tjsp_worker_1  | --- Output do Crawler ---
tjsp_worker_1  | {
tjsp_worker_1  |   "documento": null,
tjsp_worker_1  |   "processo": "0077044-50.2023.8.26.0500",
tjsp_worker_1  |   "ok": false,
tjsp_worker_1  |   "has_precatorio": false,
tjsp_worker_1  |   "found_process_numbers": [],
tjsp_worker_1  |   "results": [],
tjsp_worker_1  |   "error": "SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir\nStacktrace:\n#0 0x595b367e3fba <unknown>\n#1 0x595b362686d0 <unknown>\n#2 0x595b362a3475 <unknown>\n#3 0x595b3629f368 <unknown>\n#4 0x595b362ee280 <unknown>\n#5 0x595b362ed946 <unknown>\n#6 0x595b362dfc03 <unknown>\n#7 0x595b362ac7a8 <unknown>\n#8 0x595b362ad421 <unknown>\n#9 0x595b367a8b28 <unknown>\n#10 0x595b367ac87f <unknown>\n#11 0x595b36790c49 <unknown>\n#12 0x595b367ad405 <unknown>\n#13 0x595b367764ff <unknown>\n#14 0x595b367d1258 <unknown>\n#15 0x595b367d1432 <unknown>\n#16 0x595b367e2fa3 <unknown>\n#17 0x7f33badcf1f5 <unknown>\n\nTraceback (most recent call last):\n  File \"/app/crawler_full.py\", line 1232, in go_and_extract\n    driver = _build_chrome(\n             ^^^^^^^^^^^^^^\n  File \"/app/crawler_full.py\", line 197, in _build_chrome\n    d = webdriver.Chrome(options=opts); d.set_page_load_timeout(60); return d\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/selenium/webdriver/chrome/webdriver.py\", line 45, in __init__\n    super().__init__(\n  File \"/usr/local/lib/python3.12/site-packages/selenium/webdriver/chromium/webdriver.py\", line 66, in __init__\n    super().__init__(command_executor=executor, options=options)\n  File \"/usr/local/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py\", line 212, in __init__\n    self.start_session(capabilities)\n  File \"/usr/local/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py\", line 299, in start_session\n    response = self.execute(Command.NEW_SESSION, caps)[\"value\"]\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py\", line 354, in execute\n    self.error_handler.check_response(response)\n  File \"/usr/local/lib/python3.12/site-packages/selenium/webdriver/remote/errorhandler.py\", line 229, in check_response\n    raise exception_class(message, screen, stacktrace)\nselenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir\nStacktrace:\n#0 0x595b367e3fba <unknown>\n#1 0x595b362686d0 <unknown>\n#2 0x595b362a3475 <unknown>\n#3 0x595b3629f368 <unknown>\n#4 0x595b362ee280 <unknown>\n#5 0x595b362ed946 <unknown>\n#6 0x595b362dfc03 <unknown>\n#7 0x595b362ac7a8 <unknown>\n#8 0x595b362ad421 <unknown>\n#9 0x595b367a8b28 <unknown>\n#10 0x595b367ac87f <unknown>\n#11 0x595b36790c49 <unknown>\n#12 0x595b367ad405 <unknown>\n#13 0x595b367764ff <unknown>\n#14 0x595b367d1258 <unknown>\n#15 0x595b367d1432 <unknown>\n#16 0x595b367e2fa3 <unknown>\n#17 0x7f33badcf1f5 <unknown>\n\n",
tjsp_worker_1  |   "downloaded_files": [],
tjsp_worker_1  |   "started_at": "2025-10-01 03:01:41",
tjsp_worker_1  |   "finished_at": "2025-10-01 03:01:42",
tjsp_worker_1  |   "last_url": null,
tjsp_worker_1  |   "duration_seconds": 1.022,
tjsp_worker_1  |   "duration_hms": "0m01s"
tjsp_worker_1  | }
tjsp_worker_1  | 
tjsp_worker_1  | --- Fim do Output ---
tjsp_worker_1  | 
tjsp_worker_1  | 
tjsp_worker_1  | Atualizando status para 'processado = TRUE' para o ID: 27
tjsp_worker_1  | [SUCESSO] Status atualizado para o ID 27.
tjsp_worker_1  | Conectando ao banco de dados PostgreSQL...
tjsp_worker_1  | Executando a query para buscar o pr√≥ximo item da fila...
tjsp_worker_1  | Conex√£o com o banco de dados fechada.
tjsp_worker_1  | Nenhum precat√≥rio novo para processar encontrado. Encerrando o worker.
tjsp_worker_1 exited with code 0
root@srv987902:/opt/crawler_tjsp# 

w Enable Watch
