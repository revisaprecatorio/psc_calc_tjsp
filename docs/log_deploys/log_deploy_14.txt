‚ö†Ô∏è PROBLEMA PERSISTE - An√°lise Mais Profunda Necess√°ria
O erro continua mesmo com o diret√≥rio tempor√°rio √∫nico. Isso indica que o problema N√ÉO √© o diret√≥rio em si, mas sim processos do Chrome/Chromium que n√£o est√£o sendo encerrados.

üîç Nova Hip√≥tese:
O Chrome est√° travando/n√£o encerrando ap√≥s cada execu√ß√£o, deixando processos zombie que bloqueiam novos lan√ßamentos.

Evid√™ncia:

File "/app/crawler_full.py", line 197, in _build_chrome
O erro acontece na linha 197, que √© onde tentamos criar uma nova inst√¢ncia do Chrome.

‚úÖ Solu√ß√£o: Adicionar Limpeza de Processos
Vamos implementar a Op√ß√£o 1 que mencionei antes - matar processos do Chrome antes de cada execu√ß√£o:

orchestrator_subprocess.py
+9
~/‚Ä¶/crawler_tjsp$ git add orchestrator_subprocess.py


~/‚Ä¶/crawler_tjsp$ git commit -m "fix: adiciona limpeza de processos chrome antes de cada execu√ß√£o

- Mata processos chrome/chromium/chromedriver antes de executar crawler
- Usa pkill -9 para for√ßar encerramento de processos zombie
- Adiciona timeout de 5s para evitar travamento
- Resolve problema de processos Chrome n√£o encerrados
- Combinado com temp dir √∫nico, deve resolver definitivamente"


[main 4632426] fix: adiciona limpeza de processos chrome antes de cada execu√ß√£o
 1 file changed, 9 insertions(+)
Exit Code 0
~/‚Ä¶/crawler_tjsp$ git push origin main


‚úÖ NOVA CORRE√á√ÉO APLICADA!
üîß O Que Foi Adicionado:
orchestrator_subprocess.py - Antes de cada execu√ß√£o:
python
# NOVO: Matar processos antigos do Chrome
try:
    subprocess.run(["pkill", "-9", "chrome"], capture_output=True, timeout=5)
    subprocess.run(["pkill", "-9", "chromium"], capture_output=True, timeout=5)
    subprocess.run(["pkill", "-9", "chromedriver"], capture_output=True, timeout=5)
except Exception:
    pass  # Ignora se n√£o houver processos para matar
üéØ Como Funciona:
‚úÖ Antes de cada crawler: Mata todos os processos Chrome/Chromium/ChromeDriver
‚úÖ For√ßa encerramento: Usa -9 (SIGKILL) para garantir que processos zombie sejam mortos
‚úÖ Timeout de 5s: Evita travamento se pkill demorar
‚úÖ Ignora erros: Se n√£o houver processos, continua normalmente
üöÄ Comandos para Executar no Servidor:
bash
# 1. Parar o worker
docker compose down

# 2. Atualizar c√≥digo
git pull origin main

# 3. Rebuild
docker compose build

# 4. Resetar job
psql -h 72.60.62.124 -p 5432 -U admin -d n8n -c "UPDATE consultas_esaj SET status = FALSE WHERE id = 27;"

# 5. Subir e ver logs
docker compose up
üìä O Que Deve Acontecer Agora:
‚úÖ Processos antigos do Chrome s√£o mortos
‚úÖ Diret√≥rio tempor√°rio √∫nico √© criado
‚úÖ Chrome inicia sem conflitos
‚úÖ Selenium funciona corretamente
‚úÖ PDFs s√£o baixados
Esta combina√ß√£o de solu√ß√µes (temp dir + kill processes) deve resolver definitivamente o problema! üéâ

Execute os comandos acima e me mostre o resultado! üöÄ

--- EXEC ---

root@srv987902:/opt/crawler_tjsp# docker compose down
WARN[0000] /opt/crawler_tjsp/docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion 
[+] Running 2/2
 ‚úî Container tjsp_worker_1       Removed                                         0.0s 
 ‚úî Network crawler_tjsp_default  Removed                                         0.1s 
root@srv987902:/opt/crawler_tjsp# git pull origin main
remote: Enumerating objects: 5, done.
remote: Counting objects: 100% (5/5), done.
remote: Compressing objects: 100% (1/1), done.
remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0 (from 0)
Unpacking objects: 100% (3/3), 709 bytes | 709.00 KiB/s, done.
From https://github.com/revisaprecatorio/crawler_tjsp
 * branch            main       -> FETCH_HEAD
   33a7c78..4632426  main       -> origin/main
Updating 33a7c78..4632426
Fast-forward
 orchestrator_subprocess.py | 9 +++++++++
 1 file changed, 9 insertions(+)
root@srv987902:/opt/crawler_tjsp# docker compose build
WARN[0000] /opt/crawler_tjsp/docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion 
#1 [internal] load local bake definitions
#1 reading from stdin 496B done
#1 DONE 0.0s

#2 [internal] load build definition from Dockerfile
#2 transferring dockerfile: 1.19kB done
#2 DONE 0.0s

#3 [internal] load metadata for docker.io/library/python:3.12-slim-bookworm
#3 DONE 0.4s

#4 [internal] load .dockerignore
#4 transferring context: 2B done
#4 DONE 0.0s

#5 [1/7] FROM docker.io/library/python:3.12-slim-bookworm@sha256:25c2d6d3b4680597f42caabe8b3ade05a60bba86547cb219356d5774ab319a8a
#5 DONE 0.0s

#6 [internal] load build context
#6 transferring context: 36.92kB 0.0s done
#6 DONE 0.0s

#7 [2/7] RUN apt-get update && apt-get install -y --no-install-recommends     wget ca-certificates gnupg2 unzip fonts-liberation     libnss3-tools openssl chromium     libnss3 libxss1 libasound2 libatk1.0-0 libgtk-3-0 libgbm1 libx11-xcb1   && rm -rf /var/lib/apt/lists/*
#7 CACHED

#8 [3/7] WORKDIR /app
#8 CACHED

#9 [5/7] RUN pip install --upgrade pip
#9 CACHED

#10 [4/7] COPY requirements.txt /app/
#10 CACHED

#11 [6/7] RUN pip install -r requirements.txt
#11 CACHED

#12 [7/7] COPY . /app/
#12 DONE 0.1s

#13 exporting to image
#13 exporting layers
#13 exporting layers 0.0s done
#13 writing image sha256:129f3fc1ec0593021565b205498efe060ebd1595c705e2fadaa92f0ab3044d44 done
#13 naming to docker.io/library/tjsp-worker:latest done
#13 DONE 0.0s

#14 resolving provenance for metadata file
#14 DONE 0.0s
[+] Building 1/1
 ‚úî tjsp-worker:latest  Built                                                     0.0s 
root@srv987902:/opt/crawler_tjsp# psql -h 72.60.62.124 -p 5432 -U admin -d n8n -c "UPDATE consultas_esaj SET status = FALSE WHERE id = 27;"
Password for user admin: 
UPDATE 1
root@srv987902:/opt/crawler_tjsp# docker compose up
WARN[0000] /opt/crawler_tjsp/docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion 
[+] Running 2/2
 ‚úî Network crawler_tjsp_default  Created                                         0.1s 
 ‚úî Container tjsp_worker_1       Created                                         0.0s 
Attaching to tjsp_worker_1
tjsp_worker_1  | Conectando ao banco de dados PostgreSQL...
tjsp_worker_1  | Executando a query para buscar o pr√≥ximo item da fila...
tjsp_worker_1  | Item encontrado para processar: ID=27, CPF=07620857893
tjsp_worker_1  | Conex√£o com o banco de dados fechada.
tjsp_worker_1  | 
tjsp_worker_1  | ================================================================================
tjsp_worker_1  | Processando item 1/1 do Job ID=27: Processo 0077044-50.2023.8.26.0500
tjsp_worker_1  | ================================================================================
tjsp_worker_1  | Executando comando: /usr/local/bin/python crawler_full.py --doc 0077044-50.2023.8.26.0500 --abrir-autos --baixar-pdf --turbo-download --download-dir /app/downloads/07620857893
tjsp_worker_1  | 
tjsp_worker_1  | --- Output do Crawler ---
tjsp_worker_1  | {
tjsp_worker_1  |   "documento": null,
tjsp_worker_1  |   "processo": "0077044-50.2023.8.26.0500",
tjsp_worker_1  |   "ok": false,
tjsp_worker_1  |   "has_precatorio": false,
tjsp_worker_1  |   "found_process_numbers": [],
tjsp_worker_1  |   "results": [],
tjsp_worker_1  |   "error": "SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir\nStacktrace:\n#0 0x569aefd16fba <unknown>\n#1 0x569aef79b6d0 <unknown>\n#2 0x569aef7d6475 <unknown>\n#3 0x569aef7d2368 <unknown>\n#4 0x569aef821280 <unknown>\n#5 0x569aef820946 <unknown>\n#6 0x569aef812c03 <unknown>\n#7 0x569aef7df7a8 <unknown>\n#8 0x569aef7e0421 <unknown>\n#9 0x569aefcdbb28 <unknown>\n#10 0x569aefcdf87f <unknown>\n#11 0x569aefcc3c49 <unknown>\n#12 0x569aefce0405 <unknown>\n#13 0x569aefca94ff <unknown>\n#14 0x569aefd04258 <unknown>\n#15 0x569aefd04432 <unknown>\n#16 0x569aefd15fa3 <unknown>\n#17 0x70f367f211f5 <unknown>\n\nTraceback (most recent call last):\n  File \"/app/crawler_full.py\", line 1232, in go_and_extract\n    driver = _build_chrome(\n             ^^^^^^^^^^^^^^\n  File \"/app/crawler_full.py\", line 197, in _build_chrome\n    d = webdriver.Chrome(options=opts); d.set_page_load_timeout(60); return d\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/selenium/webdriver/chrome/webdriver.py\", line 45, in __init__\n    super().__init__(\n  File \"/usr/local/lib/python3.12/site-packages/selenium/webdriver/chromium/webdriver.py\", line 66, in __init__\n    super().__init__(command_executor=executor, options=options)\n  File \"/usr/local/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py\", line 212, in __init__\n    self.start_session(capabilities)\n  File \"/usr/local/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py\", line 299, in start_session\n    response = self.execute(Command.NEW_SESSION, caps)[\"value\"]\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py\", line 354, in execute\n    self.error_handler.check_response(response)\n  File \"/usr/local/lib/python3.12/site-packages/selenium/webdriver/remote/errorhandler.py\", line 229, in check_response\n    raise exception_class(message, screen, stacktrace)\nselenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir\nStacktrace:\n#0 0x569aefd16fba <unknown>\n#1 0x569aef79b6d0 <unknown>\n#2 0x569aef7d6475 <unknown>\n#3 0x569aef7d2368 <unknown>\n#4 0x569aef821280 <unknown>\n#5 0x569aef820946 <unknown>\n#6 0x569aef812c03 <unknown>\n#7 0x569aef7df7a8 <unknown>\n#8 0x569aef7e0421 <unknown>\n#9 0x569aefcdbb28 <unknown>\n#10 0x569aefcdf87f <unknown>\n#11 0x569aefcc3c49 <unknown>\n#12 0x569aefce0405 <unknown>\n#13 0x569aefca94ff <unknown>\n#14 0x569aefd04258 <unknown>\n#15 0x569aefd04432 <unknown>\n#16 0x569aefd15fa3 <unknown>\n#17 0x70f367f211f5 <unknown>\n\n",
tjsp_worker_1  |   "downloaded_files": [],
tjsp_worker_1  |   "started_at": "2025-10-01 03:05:07",
tjsp_worker_1  |   "finished_at": "2025-10-01 03:05:08",
tjsp_worker_1  |   "last_url": null,
tjsp_worker_1  |   "duration_seconds": 0.723,
tjsp_worker_1  |   "duration_hms": "0m01s"
tjsp_worker_1  | }
tjsp_worker_1  | 
tjsp_worker_1  | --- Fim do Output ---
tjsp_worker_1  | 
tjsp_worker_1  | 
tjsp_worker_1  | Atualizando status para 'processado = TRUE' para o ID: 27
tjsp_worker_1  | [SUCESSO] Status atualizado para o ID 27.
tjsp_worker_1  | Conectando ao banco de dados PostgreSQL...
tjsp_worker_1  | Executando a query para buscar o pr√≥ximo item da fila...
tjsp_worker_1  | Conex√£o com o banco de dados fechada.
tjsp_worker_1  | Nenhum precat√≥rio novo para processar encontrado. Encerrando o worker.
tjsp_worker_1 exited with code 0
root@srv987902:/opt/crawler_tjsp# 

w Enable Watch